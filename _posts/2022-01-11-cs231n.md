---
layout:     post
title:      CS231nçš„å­¦ä¹ ç¬”è®°
subtitle:   è¿ˆå…¥ç¥žç»ç½‘ç»œçš„ç¼ºå¾·æ²³æµ
date:       2022-01-11
author:     hdbdjdbm
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - æœºå™¨å­¦ä¹ 
---

## å‰è¨€

æ­¤æ–‡ç« ä¸º Stanford Winter Quarter 2016 class: CS231n: Convolutional Neural Networks for Visual Recognitionè¯¾ç¨‹ç¬”è®°ã€‚

ç”±äºŽåœ¨äº”å­æ£‹(MCTS)æ˜¯AlphaZeroçš„ä¸€ä¸ªæŠ›ç –å¼•çŽ‰ï¼Œå€Ÿæ­¤æœºä¼šå­¦ä¹ ä¸€ä¸‹ç¥žç»ç½‘ç»œã€‚

> äººç±»çš„æ„ŸçŸ¥50%ä»¥ä¸Šæ˜¯é€šè¿‡è§†è§‰å¤„ç†èŽ·å–çš„ã€‚

è¿™ä¸ªç»“è®ºè®©æˆ‘å›žå¿†èµ·å°å­¦ä¸‰å››å¹´çº§çš„æ—¶å€™å–œæ¬¢è·³çš®ç­‹ ;äº”å…­å¹´çº§å–œæ¬¢çŽ©4399çš„flashæ¸¸æˆ ;åˆä¸€åˆäºŒå¼€å§‹çœ‹å°è¯´ ;é«˜ä¸€é«˜äºŒå¼€å§‹çœ‹è§†é¢‘(è¿½æ˜Ÿ)ã€‚è¿™æ ·æ¢³ç†å‘çŽ°ä¸€åˆ‡éƒ½æ˜¯åœ¨æœç€â€œæ„ŸçŸ¥å¤šå…ƒåŒ–â€çš„æ–¹å‘å‘å±•ã€‚

åœ¨ä¸ä¹…ä¹‹å‰æˆ‘å°±æ„è¯†åˆ°ä¿¡æ¯èŽ·å–çš„å¤šæ¸ é“æ€§è®©å­¦ä¹ å˜å¾—æ›´å®¹æ˜“äº†(å¹¿ä¹‰çš„å­¦ä¹ )ã€‚ä»¥å‰çŸ¥è¯†çš„èŽ·å–åªèƒ½é€šè¿‡è€å¸ˆçš„ä¼ æ•™,åŒå­¦çš„åˆ†äº«ã€‚ä½†äº’è”ç½‘ä½¿å¾—â€œè¯·è€å¸ˆâ€çš„é—¨æ§›å˜ä½Žå¾ˆå¤šã€‚è¿™æ˜¯æ•™è‚²çš„æ™®ä¸–æ€§,ä»Žä¸€ä¸ªæ™®é€šäººçš„è§’åº¦æ¥çœ‹,è¿™å½“ç„¶æ˜¯å¥½çš„ã€‚

æœ€è¿‘è¿™ä¸€ä¸ªæœˆæ²‰è¿·ç§‘å¹»ç”µå½±æ— æ³•è‡ªæ‹”,ä¸€ç›´æƒ³å†™ä¸€äº›è§‚åŽæ„Ÿ,ä½†ä¸‹ç¬”çš„æ—¶å€™æ€»æ‰¾ä¸åˆ°ä¸€ä¸ªåˆé€‚çš„åˆ‡å…¥ç‚¹ã€‚ç»“æžœåœ¨å­¦ä¹ æœ‰å…³çš„çŸ¥è¯†ç‚¹çš„æ—¶å€™,åè€Œåˆæœ‰äº†å¾ˆå¤šèƒ½æŠ’å‘çš„è§’åº¦ã€‚(æ­¤å¥è¯å¯ä»¥ç¿»è¯‘ï¼šåœ¨å­¦ä¹ çš„æ—¶å€™,å…¶ä»–ä¸œè¥¿éƒ½å¾ˆæœ‰æ„æ€)

å·ç§¯æ¨¡åž‹åœ¨1998å¹´å°±å·²ç»è¢«LeCunæå‡ºã€‚ä½†å½“æ—¶ç¢äºŽæ•°æ®ä»¥åŠç¡¬ä»¶å¤§å®¶å¹¶ä¸çŸ¥é“è¿™ä¸ªæ¨¡åž‹èƒ½work outã€‚

æœ€è¿‘åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸ,ä¼šæ˜Žæ˜¾å‘çŽ°ä¸€äº›é—®é¢˜ï¼špracticeæ€»æ˜¯èµ°åœ¨theoryçš„å‰é¢ã€‚

professor Li æŠŠæ•´ä¸ªdata-drivençš„modelç±»æ¯”ä¸ºå°å­©çš„æˆé•¿è¿‡ç¨‹ã€‚çœ¼ç›çœ‹åˆ°çš„viewä¸ºè¾“å…¥é¡¹ï¼Œæˆ‘ä»¬çš„åŸºå› å°±æ˜¯å·²ç»trainå¥½çš„modelã€‚ç”±äºŽæ•°æ®çš„é«˜ç»´åŒ–ï¼Œè®©äººä¸å¾—ä¸ä½¿ç”¨å¤šé‡å·ç§¯åŽ»æ‹Ÿåˆé«˜ç»´ã€‚

My point is,å®žé™…ä¸Šè¿™ä¸ªè¿‡ç¨‹å¯ä»¥è§†ä¸ºé«˜ç»´æ•°æ®æå–ä½Žç»´ç‰¹å¾ã€‚UCBçš„professor Ma ä¹Ÿæåˆ°è¿‡è¿™ä¸ªç‚¹(å¼ºè¡Œç¢°ç“·äº†å±žäºŽæ˜¯ï¼Œå†™åœ¨todo-list)ã€‚ç±»ä¼¼äººç±»åŒºåˆ†çŒ«å’Œç‹—ï¼Œå¦‚æžœæœ‰nä¸ªç‰¹å¾ï¼šä¸€çœ‹åŽŸè¾¹å½¢çŠ¶ï¼ŒäºŒçœ‹çº¹ç†ï¼Œä¸‰çœ‹é¢œè‰²ï¼Œå››çœ‹...ã€‚åˆ°åº•å¤šå°‘çœ‹èƒ½åŒºåˆ†å‡ºçŒ«å’Œç‹—ã€‚æ¯ä¸€ä¸ªçœ‹æ˜¯å¦èƒ½ä»Žé«˜ç»´çŸ©é˜µä¸­æå–å‡ºæ¥ã€‚(å¼€å§‹å£å—¨)

å½“æˆ‘ä»¬çš„è¯†åˆ«ä»Ž0åˆ°1è¿ˆå‡ºä¸”åˆ°è¾¾äº†ä¹‹åŽ,å°±ä¼šä»Ž1-100é£žé€Ÿå¿«è¿›ï¼Œè¿˜æ˜¯æ‹¿å°æœ‹å‹ç±»æ¯”ï¼šæ­£å¸¸å°æœ‹å‹3å²èƒ½è¯†å­—ï¼Œ5å²èƒ½èƒŒè¯—ã€‚çªç„¶æœ‰ä¸ªå°æœ‹å‹3å²å°±èƒ½èƒŒè¯—,å®£å¸ƒè¿™ä¸ªç»“æžœåŽï¼Œæœ‰äº›å°æœ‹å‹2å²å°±èƒ½èƒŒè¯—(å¤¸å¼ )ã€‚ðŸ’»åœ¨ç›®å‰å‘å±•é˜¶æ®µä¸å­˜åœ¨å¤§é‡çš„ä¼¦ç†é—®é¢˜(æ­¤æ—¶å¿…é¡»æåˆ°2001ç”µå½±é‡Œé¢çš„äººå·¥æ™ºèƒ½å¯¹äººç±»è¯´çš„è¯ï¼šå¦‚æžœæœ‰é—®é¢˜ï¼Œä¸€å®šæ˜¯äººç±»çš„é—®é¢˜)ï¼Œæ‰€ä»¥äººä»¬ä¼šç«­å°½æ‰€èƒ½è®©ä¸‰ä¸ªæœˆçš„â€œå­©å­â€å­¦ä¼šèƒŒè¯—ã€‚

æˆ‘ä¸€ç›´å¯¹æ»¥ç”¨å†…å·ä¿æœ‰å¦å®šæ€åº¦ï¼Œå¯¹äºŽæ‰€æœ‰èº«å¤„å­¦æœ¯ç•Œçš„åŒå­¦ä»¬(å“ªæ€•åªæ˜¯ç¢°åˆ°äº†å­¦æœ¯ç•Œçš„ðŸšª)æ›´åº”è¯¥è®¤è¯†åˆ°è¿™ä¸€ç‚¹ã€‚å­¦æœ¯ç•Œçš„å†…å·æ˜¯äººç±»æ›´æ–°è¿­ä»£çš„å¿…é¡»æ¡ä»¶ã€‚LeCunä¹Ÿæœªæ›¾çŸ¥åå¹´åŽå·ç§¯æ¨¡åž‹ä¼šæŽ¨åŠ¨ä¸–ç•Œçš„è¿›æ­¥ï¼Œä½†å­¦æœ¯ç•Œçš„äººä»¬åº”è¯¥ä¿æœ‰ä½¿å‘½æ„Ÿï¼Œä½†ä¸è¦è®¤ä¸ºè‡ªå·±æ˜¯ä½¿å‘½çŽ‹è€…ï¼Œè¦ä¸å°±æˆå‚»é€¼äº†ã€‚

æœ€åŽï¼Œæ„Ÿè°¢professor Liåå¹´å‰çš„åšæŒï¼Œæ„Ÿè°¢æ‰€æœ‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå‘å…‰å‘çƒ­çš„äººä»¬ã€‚



### Lecture 1 Introduction

è§†è§‰ä¿¡æ¯ åƒç´ ä¿¡æ¯

"ä¸€äº›å…³é”®è¯è®°å½•"

* åŽ†å²å…ˆé©±(1981è¯ºè´å°”åŒ»å­¦å¥–)
æ¯ä¸€åˆ—çš„ç¥žç»å…ƒåªè¯†åˆ«æŸä¸€ç§ç‰¹å®šçš„è¾¹ç¼˜
* Block world(1966,MIT)
äººä»¬è¯†åˆ«ç‰©ä½“æ˜¯åŸºäºŽåŸºäºŽç‰©ä½“çš„è¾¹ç¼˜å’Œå½¢çŠ¶ã€‚
* David Marrï¼š
visual start with a simple 
visual is  HIERARCHICA(åˆ†å±‚)
-->è¾¹ç¼˜ç»“æž„ï¼Œ2.5Dï¼Œ3D
* normlized cut
* Face Detection
* features(ç‰¹å¾)
* part model
* PASCAL Visual Object Challenge
* 2012 å¹´çš„Image Classification Challengeä½¿ç”¨äº†å·ç§¯ç½‘ç»œç‰¹å¾+SVMï¼Œ7å±‚ã€‚æ€è·¯å’Œ1998Yann LeCunå‡ ä¹Žä¸€æ ·ã€‚ä½†é‚£ä¸ªæ—¶å€™ç¡¬ä»¶æœ‰é™åˆ¶ã€‚
* 2015æ·±åº¦æ®‹å·®ç½‘ç»œ(151å±‚)
  
æœ€ç»ˆæ„¿æƒ³çš„output:çœ‹å›¾è®²æ•…äº‹


### Lecture 2&3 å›¾åƒåˆ†ç±»

**The problem: semantic gap**

Eg:300 x 100 x 3
* 0-255,ä¸‰ç»´(äº®åº¦ï¼Œä½ç½®ï¼ŒRGB)æ•°æ®è®°å½•å±€éƒ¨å›¾åƒã€‚
1. ç”±äºŽäº®åº¦ä¼šå˜åŒ–ï¼Œéœ€è¦**é²æ£’æ€§**çš„ç®—æ³•å®žçŽ°ç¨³å®šæ€§
2. å˜å½¢deformation
3. background clutter ç­‰ç­‰

solution: data-driven approach

#### kNN classifier

* key point: æ¯”è¾ƒä¸¤ä¸ªå›¾åƒï¼šè·ç¦»çš„åº¦é‡ ,the choice of distance and k are hyperpearameters(è¶…å‚æ•°)
--å–å†³äºŽé—®é¢˜--è®¾ç½®fold - cross validationã€‚Split data into folds,try each fold as validation and average the resultsã€‚

* notice: Useful for small datasets, but not used too frequently in deep learning. And KNN with pixel distance never used: (1) ä¸€æ–¹é¢åœ¨é«˜ç»´åº¦è·ç¦»æœ¬èº«æ²¡æœ‰æ„ä¹‰(å¯è§ä¸Šä¼ çš„å¦ä¸€ç¯‡final paper)ã€‚(2) å¹¶ä¸”è·ç¦»æä¾›çš„åº¦é‡ä¸å¤ªå‡†ç¡®ã€‚(3) testæ—¶é—´æ¯”è¾ƒé•¿

* features
(1) è·ç¦»çš„åº¦é‡æœ‰ä¸åŒçš„é€‰æ‹©ï¼Œæ¯”å¦‚ $L_1$ å’Œ $L_2$ ã€‚éœ€è¦æ³¨æ„ä¸åŒè·ç¦»çš„æ€§è´¨ã€‚
(2) åœ¨å¯¹çŸ©é˜µè¿›è¡Œå¤„ç†çš„æ—¶å€™æ³¨æ„numpyçš„å¤„ç†ï¼Œæ¯”å¦‚axiså’Œkeepdimï¼Œè¿˜æœ‰ä¸€äº›ç®€ä¾¿è¿ç®—çš„å‡½æ•°æ¯”å¦‚bincount,argmaxç­‰ã€‚
(3) KNNå¹¶ä¸æ˜¯çº¿æ€§åˆ†ç±»å™¨ï¼Œè€Œæ˜¯å°†ç©ºé—´åˆ†ä¸ºå‡¸å¤šè¾¹å½¢ã€‚

* å¯ä¾›æ‹“å±•: å¯¹äºŽKNNåˆ†ç±»å™¨æ¥è¯´ã€‚trainéœ€è¦O(1),ä½†æ˜¯predictéœ€è¦ O(N). å¾ˆBADï¼Œæ‰€ä»¥æœ‰ä¸€äº›fast/approximateçš„æ–¹æ³•ï¼Œæ¯”å¦‚k-d treeæˆ–è€… ball- tree.


#### çº¿æ€§åˆ†ç±»

![image-1](https://pic.imgdb.cn/item/61dd3a492ab3f51d91818262.png)

![image-2](https://pic.imgdb.cn/item/61dd3a492ab3f51d91818266.png)

W: reshapingï¼Œåªæ˜¯åœ¨è®¡æ•°ï¼Œæ¶ˆé™¤å¤±çœŸã€‚ 

* key point: ç”¨çº¿æ€§å‡½æ•°åˆ†éš”ç©ºé—´

* features
Interpreting a Linear Classifierï¼šWæ˜¯ç±»åˆ«çš„çŸ©é˜µ

* quantifing what it seems to have a god W:Loss function

#### SVM loss

the SVM loss has the form:

$$
L_i = \sum_{j\neq y_i}max(0,s_j - s_{y_i} + 1)
$$

$$
f(x,W)=  Wx
$$

$$
L = \frac{1}{N}\sum_{i=1}^N\sum_{j\neq y_i}max(0,f(x_i;W)_j-f(x_i;W)_{y_i} +1 )
$$

w is not unique, we want regularization:

$$
L(W) = \frac{1}{N}\sum_{i=1}^{N}L_i(f(x_i,W),y_i) + \lambda R(W)
$$


å‰è€…æ˜¯Data loss: Model predictions
should match training data

åŽè€…æ˜¯Regularization: Prevent the model
from doing too well on training dataï¼Œ$\lambda$ is regularization strength
(hyperparameter)ã€‚$R(W)$ = $\sum\sum w^2$





#### softmax loss(Multinamial Logistic Regression)

$$
s = f(x_i;W)
$$

$$
P(Y=k|X=x_i) = \frac{e^sk}{\sum_j e^{sj}}
$$

$$
L_i = -log(\frac{e^{sy_i}}{\sum_j e^{sj}})
$$

**SVM loss VS softmax loss**



* å¦‚ä½•æœ€å¿«çš„æ‰¾åˆ°å‘¢ï¼š
#### Optimization

numerical fradient
&
analytical gradient

> always use the analytical gradient but to check the numerical gradient this is called gradient check


Stochastic Gradient Descent (SGD)

Approximate sum
using a minibatch of examples

### Lecture 4 Neural Networks and Backpropagation

questionï¼šHow to find the best W $\triangledown_WL$


#### features of pictures
Color Histogram and Histogram of Oriented Gradients (HoG)

#### Neural Networks

* defination

![image-5](https://pic.imgdb.cn/item/61eebaff2ab3f51d9195a978.png)


* activation function
![image-6](https://pic.imgdb.cn/item/61eebbb32ab3f51d91966888.png)
(ReLU is a good default
choice for most problems)

* training a 2-layer Neural Network needs:Define the network,Forward pass,Calculate the analytical gradients

* Setting the number of layers and their sizes
![image-7](https://pic.imgdb.cn/item/61eebd4c2ab3f51d91980d2a.png)

![image-8](https://pic.imgdb.cn/item/61eebd4c2ab3f51d91980d31.png)

$$
s = f(x;W_1,W_2) = W_2max(0,W_1x)
$$ 
Nonlinear score function
$$
L_i = \sum_{j\neq y_i}max(0,s_j - s_{y_i} + 1)
$$
SVM Loss on predictions
$$
R(W) = \sum_k W_k^2
$$
Regularization
$$
L = \frac{1}{N}\sum_{i=1}^N L_i + \lambda R(W_1) + \lambda R(W_2)
$$

question: compute the gradient of $W_1$ and $W_2$
solution: Backpropagation

![image-9](https://pic.imgdb.cn/item/61eebf462ab3f51d919a43de.png)

