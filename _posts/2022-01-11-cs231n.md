---
layout:     post
title:      CS231n的学习笔记
subtitle:   迈入神经网络的缺德河流
date:       2022-01-11
author:     hdbdjdbm
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - 机器学习
---

## 前言

此文章为 Stanford Winter Quarter 2016 class: CS231n: Convolutional Neural Networks for Visual Recognition课程笔记。

由于在五子棋(MCTS)是AlphaZero的一个抛砖引玉，借此机会学习一下神经网络。

> 人类的感知50%以上是通过视觉处理获取的。

这个结论让我回忆起小学三四年级的时候喜欢跳皮筋 ;五六年级喜欢玩4399的flash游戏 ;初一初二开始看小说 ;高一高二开始看视频(追星)。这样梳理发现一切都是在朝着“感知多元化”的方向发展。

在不久之前我就意识到信息获取的多渠道性让学习变得更容易了(广义的学习)。以前知识的获取只能通过老师的传教,同学的分享。但互联网使得“请老师”的门槛变低很多。这是教育的普世性,从一个普通人的角度来看,这当然是好的。

最近这一个月沉迷科幻电影无法自拔,一直想写一些观后感,但下笔的时候总找不到一个合适的切入点。结果在学习有关的知识点的时候,反而又有了很多能抒发的角度。(此句话可以翻译：在学习的时候,其他东西都很有意思)

卷积模型在1998年就已经被LeCun提出。但当时碍于数据以及硬件大家并不知道这个模型能work out。

最近在机器学习领域,会明显发现一些问题：practice总是走在theory的前面。

professor Li 把整个data-driven的model类比为小孩的成长过程。眼睛看到的view为输入项，我们的基因就是已经train好的model。由于数据的高维化，让人不得不使用多重卷积去拟合高维。

My point is,实际上这个过程可以视为高维数据提取低维特征。UCB的professor Ma 也提到过这个点(强行碰瓷了属于是，写在todo-list)。类似人类区分猫和狗，如果有n个特征：一看原边形状，二看纹理，三看颜色，四看...。到底多少看能区分出猫和狗。每一个看是否能从高维矩阵中提取出来。(开始口嗨)

当我们的识别从0到1迈出且到达了之后,就会从1-100飞速快进，还是拿小朋友类比：正常小朋友3岁能识字，5岁能背诗。突然有个小朋友3岁就能背诗,宣布这个结果后，有些小朋友2岁就能背诗(夸张)。💻在目前发展阶段不存在大量的伦理问题(此时必须提到2001电影里面的人工智能对人类说的话：如果有问题，一定是人类的问题)，所以人们会竭尽所能让三个月的“孩子”学会背诗。

我一直对滥用内卷保有否定态度，对于所有身处学术界的同学们(哪怕只是碰到了学术界的🚪)更应该认识到这一点。学术界的内卷是人类更新迭代的必须条件。LeCun也未曾知十年后卷积模型会推动世界的进步，但学术界的人们应该保有使命感，但不要认为自己是使命王者，要不就成傻逼了。

最后，感谢professor Li十年前的坚持，感谢所有在人工智能领域发光发热的人们。



### Lecture 1 Introduction

视觉信息 像素信息

"一些关键词记录"

* 历史先驱(1981诺贝尔医学奖)
每一列的神经元只识别某一种特定的边缘
* Block world(1966,MIT)
人们识别物体是基于基于物体的边缘和形状。
* David Marr：
visual start with a simple 
visual is  HIERARCHICA(分层)
-->边缘结构，2.5D，3D
* normlized cut
* Face Detection
* features(特征)
* part model
* PASCAL Visual Object Challenge
* 2012 年的Image Classification Challenge使用了卷积网络特征+SVM，7层。思路和1998Yann LeCun几乎一样。但那个时候硬件有限制。
* 2015深度残差网络(151层)
  
最终愿想的output:看图讲故事


### Lecture 2&3 图像分类

**The problem: semantic gap**

Eg:300 x 100 x 3
* 0-255,三维(亮度，位置，RGB)数据记录局部图像。
1. 由于亮度会变化，需要**鲁棒性**的算法实现稳定性
2. 变形deformation
3. background clutter 等等

solution: data-driven approach

#### kNN classifier

* key point: 比较两个图像：距离的度量 ,the choice of distance and k are hyperpearameters(超参数)
--取决于问题--设置fold - cross validation。Split data into folds,try each fold as validation and average the results。

* notice: Useful for small datasets, but not used too frequently in deep learning. And KNN with pixel distance never used: (1) 一方面在高维度距离本身没有意义(可见上传的另一篇final paper)。(2) 并且距离提供的度量不太准确。(3) test时间比较长

* features
(1) 距离的度量有不同的选择，比如 $L_1$ 和 $L_2$ 。需要注意不同距离的性质。
(2) 在对矩阵进行处理的时候注意numpy的处理，比如axis和keepdim，还有一些简便运算的函数比如bincount,argmax等。
(3) KNN并不是线性分类器，而是将空间分为凸多边形。

* 可供拓展: 对于KNN分类器来说。train需要O(1),但是predict需要 O(N). 很BAD，所以有一些fast/approximate的方法，比如k-d tree或者 ball- tree.


#### 线性分类

![image-1](https://pic.imgdb.cn/item/61dd3a492ab3f51d91818262.png)

![image-2](https://pic.imgdb.cn/item/61dd3a492ab3f51d91818266.png)

W: reshaping，只是在计数，消除失真。 

* key point: 用线性函数分隔空间

* features
Interpreting a Linear Classifier：W是类别的矩阵

* quantifing what it seems to have a god W:Loss function

#### SVM loss

the SVM loss has the form:

$$
L_i = \sum_{j\neq y_i}max(0,s_j - s_{y_i} + 1)
$$

$$
f(x,W)=  Wx
$$

$$
L = \frac{1}{N}\sum_{i=1}^N\sum_{j\neq y_i}max(0,f(x_i;W)_j-f(x_i;W)_{y_i} +1 )
$$

w is not unique, we want regularization:

$$
L(W) = \frac{1}{N}\sum_{i=1}^{N}L_i(f(x_i,W),y_i) + \lambda R(W)
$$


前者是Data loss: Model predictions
should match training data

后者是Regularization: Prevent the model
from doing too well on training data，$\lambda$ is regularization strength
(hyperparameter)。$R(W)$ = $\sum\sum w^2$





#### softmax loss(Multinamial Logistic Regression)

$$
s = f(x_i;W)
$$

$$
P(Y=k|X=x_i) = \frac{e^sk}{\sum_j e^{sj}}
$$

$$
L_i = -log(\frac{e^{sy_i}}{\sum_j e^{sj}})
$$

**SVM loss VS softmax loss**



* 如何最快的找到呢：
#### Optimization

numerical fradient
&
analytical gradient

> always use the analytical gradient but to check the numerical gradient this is called gradient check


Stochastic Gradient Descent (SGD)

Approximate sum
using a minibatch of examples

### Lecture 4 Neural Networks and Backpropagation

question：How to find the best W $\triangledown_WL$


#### features of pictures
Color Histogram and Histogram of Oriented Gradients (HoG)

#### Neural Networks

* defination

![image-5](https://pic.imgdb.cn/item/61eebaff2ab3f51d9195a978.png)


* activation function
![image-6](https://pic.imgdb.cn/item/61eebbb32ab3f51d91966888.png)
(ReLU is a good default
choice for most problems)

* training a 2-layer Neural Network needs:Define the network,Forward pass,Calculate the analytical gradients

* Setting the number of layers and their sizes
![image-7](https://pic.imgdb.cn/item/61eebd4c2ab3f51d91980d2a.png)

![image-8](https://pic.imgdb.cn/item/61eebd4c2ab3f51d91980d31.png)

$$
s = f(x;W_1,W_2) = W_2max(0,W_1x)
$$ 
Nonlinear score function
$$
L_i = \sum_{j\neq y_i}max(0,s_j - s_{y_i} + 1)
$$
SVM Loss on predictions
$$
R(W) = \sum_k W_k^2
$$
Regularization
$$
L = \frac{1}{N}\sum_{i=1}^N L_i + \lambda R(W_1) + \lambda R(W_2)
$$

question: compute the gradient of $W_1$ and $W_2$
solution: Backpropagation

![image-9](https://pic.imgdb.cn/item/61eebf462ab3f51d919a43de.png)

