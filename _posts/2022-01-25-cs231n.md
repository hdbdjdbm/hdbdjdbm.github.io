---
layout: post
title: the note of cs231n
date: 2022-01-25 17:39:00
description: 学习笔记
---

### 前言

此篇为 Stanford Winter Quarter 2016 class: CS231n: Convolutional Neural Networks for Visual Recognition课程个人笔记。

五子棋(MCTS)是AlphaZero的一个抛砖引玉，借此机会学习一下神经网络。

> 人类的感知50%以上是通过视觉处理获取的。

这个结论让我回忆起小学三四年级的时候喜欢跳皮筋 ;五六年级喜欢玩4399的flash游戏 ;初一初二开始看小说 ;高一高二开始看视频(追星)。这样梳理发现一切都是在朝着“感知多元化”的方向发展。

卷积模型在1998年就已经被LeCun提出。但当时碍于数据以及硬件大家并不知道这个模型能work out。

Professor Li(FeiFei Li) 把整个data-driven的model类比为小孩的成长过程。眼睛看到的view为输入项，我们的基因就是已经train好的model。由于数据的高维化，让人不得不使用多重卷积去拟合高维。

在听完课程之后，发现了机器学习领域的现状：practice总是走在theory的前面。

我们常说神经网络是个“黑箱”，大致有两层意思。一个是说，我们不能刻画网络具体是在做什么（比如在每一层提取什么样的特征）。另一个是说，我们不知道它为什么在做这些，为什么会有效。

有网友拿牛顿被苹果砸中类比

> 深度学习显然是做对了something, 在某个角度触碰到了真理，但如果不打开黑箱，我们无法知道它到底做对了什么，真理是什么。在牛顿之前，大家都见到了苹果落地。但在当时人们的视角中他们一定认为，苹果落地不是很自然的吗，需要解释、需要知道为什么吗？当时的人们也会认为解释这种现象简直无从下手。跟今天的深度学习有点像吧？但是当牛顿告诉我们为什么苹果会落地之后，世界从此就不一样了。

深度学习目前就好比实验物理，有很多实验观察，但没有理论能够解释。也跟物理学的发展一样，实验总是走在前面。

随着人们接触知识的门槛越来越低(这是好事)，不断有很多提升效果的trick出现，比如Relu替换sigmod函数， batch traning和 normalization提升判别能力，drop out ,利用一些loss fuction等等。有些无谓的提升来源于经验，但是有一些来源于人类已经构建的认知。

实践和理论是分不开的，实践促进对理论的思考，理论引导实践的发展。就像学界和工业界的关系。我们既要展示一个东西能work out, 更讨论为什么能work out。

我的现有感觉是：这个过程是从高维数据提取低维特征。UCB的professor Ma 也提到过这个点(强行碰瓷了属于是，写在todo-list, Mayi老师的高维数据与低维模型)。类似人类区分猫和狗，如果有n个特征：一看原边形状，二看纹理，三看颜色，四看...。到底多少看能区分出猫和狗,每一个看是否能从高维矩阵中提取出来。当然这并不是容易的。CS231n后面的一个lecture就在讨论每一层的意义是看不太出来的(transformer改进了这一点)。

当我们的识别从0到1迈出且到达了之后,就会从1-100飞速快进，还是拿小朋友类比：正常小朋友3岁能识字，5岁能背诗。突然有个小朋友3岁就能背诗,宣布这个结果后，有些小朋友2岁就能背诗(夸张)。💻在目前发展阶段不存在大量的伦理问题(此时必须提到2001太空漫游电影里面的HAL9000对人类说的话：如果有问题，一定是人类的问题)，所以人们会竭尽所能让三个月的“孩子”学会背诗。

最近这一个月沉迷科幻电影无法自拔,一直想写一些观后感,但下笔的时候总找不到一个合适的切入点。结果在学习有关的知识点的时候,反而又有了很多能抒发的角度。(此句话可以翻译：在学习的时候,其他东西都很有意思)

另外，我一直对滥用内卷保有否定态度，对于所有身处学术界的同学们(哪怕只是碰到了学术界的大门)更应该认识到这一点。在前人基础框架打好之后后人要做的就是stand on the shoulders of giant.学术界的内卷是人类更新迭代的必要条件。LeCun也未曾知十年后卷积模型会推动世界的进步，我们应该保有使命感(但要清楚认知自己也是普通人)。

最后，感谢所有在人工智能领域发光发热的人们。


### Lecture 1 Introduction

视觉信息 像素信息

"一些关键词记录"

* 历史先驱(1981诺贝尔医学奖)
每一列的神经元只识别某一种特定的边缘
* Block world(1966,MIT)
人们识别物体是基于基于物体的边缘和形状。
* David Marr：
visual start with a simple 
visual is  HIERARCHICA(分层)
-->边缘结构，2.5D，3D
* normlized cut
* Face Detection
* features(特征)
* part model
* PASCAL Visual Object Challenge
* 2012 年的Image Classification Challenge使用了卷积网络特征+SVM，7层。思路和1998Yann LeCun几乎一样。但那个时候硬件有限制。
* 2015深度残差网络(151层)
  
最终愿想的output:看图讲故事(RNN)


### Lecture 2&3 图像分类

**The problem: semantic gap**

Eg:300 x 100 x 3
* 0-255,三维(亮度，位置，RGB)数据记录局部图像。
1. 由于亮度会变化，需要**鲁棒性**的算法实现稳定性
2. 变形deformation
3. background clutter 等等

solution: data-driven approach

#### kNN classifier

* key point: 比较两个图像：距离的度量 ,the choice of distance and k are hyperpearameters(超参数)
--取决于问题--设置fold - cross validation。Split data into folds,try each fold as validation and average the results。

* notice: Useful for small datasets, but not used too frequently in deep learning. And KNN with pixel distance never used: (1) 一方面在高维度距离本身没有意义(可见上传的另一篇final paper)。(2) 并且距离提供的度量不太准确。(3) test时间比较长

* features
   (1) 距离的度量有不同的选择，比如 $L_1$ 和 $L_2$ 。需要注意不同距离的性质。
   (2) 在对矩阵进行处理的时候注意numpy的处理，比如axis和keepdim，还有一些简便运算的函数比如bincount,argmax等。
   (3) KNN并不是线性分类器，而是将空间分为凸多边形。

* 可供拓展: 对于KNN分类器来说。train需要O(1),但是predict需要 O(N). 很BAD，所以有一些fast/approximate的方法，比如k-d tree或者 ball- tree.


#### 线性分类

![image-1](https://pic.imgdb.cn/item/61dd3a492ab3f51d91818262.png)

![image-2](https://pic.imgdb.cn/item/61dd3a492ab3f51d91818266.png)

W: reshaping，只是在计数，消除失真。 

* key point: 用线性函数分隔空间

* features
Interpreting a Linear Classifier：W是类别的矩阵

* quantifing what it seems to have a god W:Loss function

#### SVM loss

the SVM loss has the form:

$$
L_i = \sum_{j\neq y_i}max(0,s_j - s_{y_i} + 1)
$$

$$
f(x,W)=  Wx
$$

$$
L = \frac{1}{N}\sum_{i=1}^N\sum_{j\neq y_i}max(0,f(x_i;W)_j-f(x_i;W)_{y_i} +1 )
$$

w is not unique, we want regularization:

$$
L(W) = \frac{1}{N}\sum_{i=1}^{N}L_i(f(x_i,W),y_i) + \lambda R(W)
$$


前者是Data loss: Model predictions
should match training data

后者是Regularization: Prevent the model
from doing too well on training data，$\lambda$ is regularization strength
(hyperparameter)。$R(W)$ = $\sum\sum w^2$





#### softmax loss(Multinamial Logistic Regression)

$$
s = f(x_i;W)
$$

$$
P(Y=k|X=x_i) = \frac{e^sk}{\sum_j e^{sj}}
$$

$$
L_i = -log(\frac{e^{sy_i}}{\sum_j e^{sj}})
$$

**SVM loss VS softmax loss**



* 如何最快的找到呢：
#### Optimization

numerical fradient
&
analytical gradient

> always use the analytical gradient but to check the numerical gradient this is called gradient check


Stochastic Gradient Descent (SGD)

Approximate sum
using a minibatch of examples

### Lecture 4 Neural Networks and Backpropagation

question：How to find the best W $\triangledown_WL$


#### features of pictures
Color Histogram and Histogram of Oriented Gradients (HoG)

#### Neural Networks

* defination

分层计算

![image-5](https://pic.imgdb.cn/item/61eebaff2ab3f51d9195a978.png)


* activation function

激活函数

![image-6](https://pic.imgdb.cn/item/61eebbb32ab3f51d91966888.png)
(ReLU is a good default
choice for most problems)

* training a 2-layer Neural Network needs:Define the network,Forward pass,Calculate the analytical gradients

* Setting the number of layers and their sizes
![image-7](https://pic.imgdb.cn/item/61eebd4c2ab3f51d91980d2a.png)

![image-8](https://pic.imgdb.cn/item/61eebd4c2ab3f51d91980d31.png)

$$
s = f(x;W_1,W_2) = W_2max(0,W_1x)
$$ 
Nonlinear score function
$$
L_i = \sum_{j\neq y_i}max(0,s_j - s_{y_i} + 1)
$$
SVM Loss on predictions
$$
R(W) = \sum_k W_k^2
$$
Regularization
$$
L = \frac{1}{N}\sum_{i=1}^N L_i + \lambda R(W_1) + \lambda R(W_2)
$$

question: compute the gradient of $W_1$ and $W_2$
solution: Backpropagation

![image-9](https://pic.imgdb.cn/item/61eebf462ab3f51d919a43de.png)



### Lecture 5 setup notice

#### Mini-batch SGD
![image-5](https://pic.imgdb.cn/item/61f3eaaa2ab3f51d912c093d.png)
1. Sample a batch of data
2. Forward prop it through the graph, get loss
3. Backprop to calculate the gradients
4. Update the parameters using the gradient

#### activation functions
1. Sigmoid

   $$
   \sigma(x) = \frac{1}{1+e^{-x}}
   $$


   festure:值域在[0,1]
   DISAD: 
   Saturated neurons “kill” the gradients; Sigmoid outputs are not zerocentered(如果输入是+的,那么gradient也是正的); 
   exp() is a bit compute expensive;

2. tanh(x)
   Squashes numbers to range [-1,1]
   zero centered (nice)
   still kills gradients when saturated



3. ReLU(Rectified Linear Unit)
   
   $$
   f(x) = max(0,x)
   $$

   AD:
   Converges much faster than sigmoid/tanh in practice

   DISAD:
   not zero-centered output
   an annoyance

4. Leaky ReLU
   
   $$
   f(x) = max(0.01x,x)
   $$

   Parametric Rectifier (PReLU):
   $$
   f(x) = max(ax,x)
   $$

5. ELU
   
   $$
   if x>0: x
   if x <= 0: \alpha(exp(x)-1)
   $$

6. Maxout 
   $max(w_1^Tx+b_1,w_2^Tx+b_2)$

summary:
- Use ReLU. Be careful with your learning rates
- Try out Leaky ReLU / Maxout / ELU
- Try out tanh but don’t expect much
- **Don’t use sigmoid**

#### data processing

some examples

![image-10](https://pic.imgdb.cn/item/61f3e2c42ab3f51d912255a8.png)

![image-11](https://pic.imgdb.cn/item/61f3e2c42ab3f51d912255ad.png)

* it's common to zero-centered data but not normalized(why?)
* use PCA and whitening(common in ML not in image)
* substract the maen image - substract per-cha nel mean

#### weight initialization

* W=0 is a bad idea cause每一个神经元表现的一样，不好寻找gradient
  
* instaed W = 0.01 * np.random.randn(D,H) (对small networks are okay, but can lead to non-homogeneous distributions of activsations across the layers of a network)

* when debug, sometimes try 0.1* np.random,rand(D,H)

#### Batch Normalization

![image-13](https://pic.imgdb.cn/item/61f3eaaa2ab3f51d912c0941.png)

AD:Improves gradient flow ;Allows higher learning rates ;Reduces the strong dependence on initialization ;Acts as a form of regularization ;slightly reduces the need for dropout

需要注意和data processing 的normlize都是在normlize, 目的不一样,前者是为了SGD


step to train :
* Step 1: Preprocess the data
* Step 2: Choose the architecture
  Double check that the loss is reasonable
  loss went up, good. (sanity check)


#### babysitting the learning process

* Double check that the loss is reasonable:
* Make sure that you can overfit very small portion of the training data(100% accurancy we said overfit the data)

tips:

loss not going down:learning rate too low

loss exploding:learning rate too high

#### Hyperparameter Optimization

Cross-validation strategy

reg and lr: it’s best to optimize
in log space


* gap between training and validation:
   big gap = overfitting
   => increase regularization strength

   no gap
   => increase model capacity?

* Evaluation
model ensembles

### Lecture 6 continue setup notice

#### parameter update

一些个下降方法：
* momentum update(soleve the SGD slow)
* Nesterov Monmentum update
* nag
* AdaGrad update
* PMSProp update
* Adam update(looks a bit like RMSProp with momentum)
* second order optimization methods(no hyperparameters)
* BFGS(Quasi-Newton methods)
* L-BFGS(some features)

**summary**
Adam is good default choice

If you can afford to do full batch updates then try out
L-BFGS 

#### Evaluation:
Model Ensembles

Train multiple independent models,At test time average their results

> Enjoy 2% extra performance

#### Regularization (dropout)
![image-1](https://pic.imgdb.cn/item/61f3ef922ab3f51d91318849.png)

reason:
* Forces the network to have a redundant representation
* Dropout is training a large ensemble of models (that share parameters).

drop in forward pass,scale at test time

#### explanation about the parameter

learning rate(决定了我们训练网络速率的快慢,how which step we used?)


regularization(dropout)
* no need so many features
* 另一种形式的ensemble
*  Inberted fropout

#### Convolutional Neural Networks


### Lecture 7 convolution neural network的结构


#### Convolution Layer

一些看图说话环节：

反正就是多维空变换,找到一些特征

![image-1](https://pic.imgdb.cn/item/61f3fc9a2ab3f51d9140f0cf.png)

![image-2](https://pic.imgdb.cn/item/61f3fc9a2ab3f51d9140f0d3.png)

![image-3](https://pic.imgdb.cn/item/61f3fc9a2ab3f51d9140f076.png)

![image-4](https://pic.imgdb.cn/item/61f3fc9a2ab3f51d9140f07d.png)


A closer look at spatial dimensions:可以change一些stride和pad, 由于会压缩, 可以在边缘加一些0

#### Pooling layer

* makes the representations smaller and more manageable
* operates over each activation map independently
![image-2](https://pic.imgdb.cn/item/61f3fef32ab3f51d9143f1bc.png)


#### fully connected layer
Contains neurons that connect to the entire input volume, as in ordinary Neural Networks


take a look:

![image-3](https://pic.imgdb.cn/item/61f3fef32ab3f51d9143f1b8.png)

#### case study

LeNet(1998)
AlexNet(2012)
ZFNet(2013)
VGG(2014)
GooLeNet(2014)
ResNet(2015)

all there are used to clssification

### Lecture 8 Localization and Detection

![image-1](https://pic.imgdb.cn/item/61f4017d2ab3f51d91473cc5.png)

#### claassification + Localization

* classification
input:image
output:class label
evaluation metric:accuracy

* Localization

input:image
output:box in the image(x,y,w,h)
evaluation metric:intersection over the Union
Classification + Localization: Do both


Idea #1: Localization as Regression

使用回归output box coordinate

![image-1](https://pic.imgdb.cn/item/61f4031e2ab3f51d91497e1b.png)


input:image --(Neural Net)--> output:Box coordinates(4 numbers)

compare with Correct output:box coordinates(4 numbers)


Idea #2 sliding window

使用窗口分类

![image-2](https://pic.imgdb.cn/item/61f4031e2ab3f51d91497e12.png)

* Run classification + regression network at multiple locations on a highresolution image


#### detection

如果使用regression则参数就太多了

Detection as Classification

Problem: Need to test many positions and scales

Solution: If your classifier is fast enough, just do it

Before CNNs:

2005 Histogram of Oriented Gradients(HoG)

2010 deformable parts modek(DPM)

* detection as clssification
region proposals: selective search


putting it together:R-CNN
step:
1. Train (or download) a classification model for ImageNet(AlexNet)
2. Fine-tune model for detection
3. extract features
4. Train one binary SVM per class to classify region features
5. bbox regression

Evaluation: mAP

problem:slow at test time,

solution:
fast R-CNN
fasyer R-CNN

### Lecture 9 deep understanding

layers越往后越不好解释

t-SNE visualization

Decinv approaches 

optimaize the image

一些trick

#### deep dream

#### Neural Style

#### Adversarial Examples

### Lecture 10 RNN

这一块暂略, 暂时永不太上。

#### Recurrent Neural Networks


![image-1](https://pic.imgdb.cn/item/61f40be02ab3f51d9155e8c9.png)

one to one: Vanilla Neural Networks

one to many: Image Captioning(image -> sequence of words)

many to one: Sentiment Classification sequence of words -> sentiment

many to many:Machine Translation(seq of words -> seq of words)

many to many: Video classification on frame level

#### LSTM

Long short Term Memory(1997)



### Lecture 11 practice with CNN


#### data augmentation
(useful for small dataset)
* horizontal flips
* random crops/scales
* color jitter

#### transfer learning
can freeze some parameter just train the last few layers

![image-1](https://pic.imgdb.cn/item/61f40f292ab3f51d915aad27.png)

![image-2](https://pic.imgdb.cn/item/61f40f292ab3f51d915aad2b.png)

#### how to stack the convolution

* Layers :change the size of 仿射结果

* im2col

#### computing convolution
* n2cool:easy to implement, but big memory overhead
* FFT:big speeduos for small kernels
* fast algorithms seem promising, not wideljy used

#### GPU,CPU

并行和浮点数


### Lecture 12 sogtware packages

#### Caffe

from U.C. Berkely

Main classes 
* Blob: stores data and derivatives
* Layer:Trandforms bottom blobs to top blobs
* Net: many layers; computes gradients
* solver:uses gradient toupdate weights

steps:
1. convert data
2. define net
   write prototxt
3. define solver
   write prototxt
4. train

caffe:model zoo

Pros/Cons
+ good for feedforward
- need to write C++
- not good recurrent networks

#### Torch

from NYU
wriiten in C and Lua

Torch: Tensors

Torch: nn
nn module lets you easily build and train neural nets
picture

Torch: cunn
picture

Torch: optim
picture

Torch: Modules

writing your own modules is easy

Torch: nngraph

Pros/Cons
- not great for RNNs
- 


#### Theano

#### TensorFlow

from Google

TensorFlow:Tensorboard
way to visualize what's happening inside your odels



### Lecture 13 

#### Segmentation

pictures

* semantic segmentation
label every pixel in the image(repeat for clssification for every picel)
unsample
unpooling
skip connections can help

* instance segmentation
very similiar to R-CNN
detect instance, generate mask
similar pipelines to object detection
 
region classification
region refinement

do logistic regression to classify


#### soft attention


soft attention for translation
soft attention for other field

#### hard attention
pass
need reinforcement learning

### Lecture 14 Videos and Unsupervised Learning
not done yet


### assignment1

#### Q1 K-Nearest Neighbor classifier

```python
class KNearestNeighbor(object):
    def __init__(self):
        pass

    def train(self,X,y):
        self.X_train = X
        self.y_train = y

    def predict(self, X, k = 1, num_loops = 0):
        if num_loops == 0:
            dists = self.compute_distances_no_loops(X)
        if num_loops == 1:
            pass
        return self.predict_labels(sidts, k=k)
    
    def compute_distances_two_loops(self, X):
        pass

    def predict_labels(self,dists,k=1):
        pass
```

注意numpy的应用和boardcasting, **少用循环**。


#### Q2&3 Training a Support Vector Machine & Implement a Softmax classifier

> subtract the mean image from train and test data


```python

class LinearClassifier(object):
    def __init__(self):
        self.W = None
    def train(self, X, y, learning_rate=1e-3, reg=1e-5,num_iters=100,batch_size=200, verbose=False):
        pass
    
    def predict(self,X):
        pass

    def loss(self, X_batch, y_batch, reg):
        pass



class LinearSVM(LinearClassifier):
    def loss(self, X_batch, y_batch, reg):
        return svm_loss_vectorized(self, X_batch, y_batch, reg)
    
    
class Softmax(LinearClassifier):
    def loss(self, X_batch, y_batch, reg):
        return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)



```

同样, 注意向量的应用.


#### Q4 Two-Layer Neural Network

```python

class TwoLayerNet(object):
    def __init__(self,input_size, hidden_size, output_size, std=1e-4):

        self.params = {}
        self.params['W1'] = std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def loss(self, X, y=None, reg=0.0):
        pass

    def train(self, X, y, X_val, y_val,learning_rate=1e-3, learning_rate_decay=0.95,reg=5e-6, num_iters=100,batch_size=200, verbose=False):
        pass

    def prefict(self,X):
        pass
        
```

注意反向传播的part.

#### Q5 Higher Level Representations: Image Features

hog_feature 
color_histogram_hsv

### assignment2


#### Q1 Multi-Layer Fully Connected Neural Networks 

```python

class TwoLayerNet(object):
    pass
class FullyConnectedNet(object):
    pass
```

train又是另一个专门的类，cause model有很多, 但train的动作都是相似的。

#### Q2 Batch Normalization 

> with batch normalization we can avoid the problem of vanishing and exploding gradients because it normalizes every affine layer (xW+b), avoiding very large/small values. Moreover, its regularization properties allow to decrease overfitting.

> the batch size affects directly the performance of batch normalization (the smaller the batch size the worse). Even the baseline model outperforms the batchnorm model when using a very small batch size. This problem occurs because when we calculate the statistics of a batch, i.e., mean and variance, we try to find an approximation of the statistics of the entire dataset. Therefore with a small batch size, these statistics can be very noisy. On the other hand, with a large batch size we can obtain a better approximation.

other idea: **Layer Normalization**


#### Q3 Dropout 

dropout可以减少一些过拟合现象

#### Q4 Convolutional Neural Networks

记录一些小trick

#### Q5 PyTorch/TensorFlow on CIFAR-10 

We want you to stand on the shoulders of giants!


### assingment3

这一块暂时还没有目的性, 后续补

#### Q1: Image Captioning with Vanilla RNNs (30 points)

#### Q2: Image Captioning with Transformers (20 points)

#### Q3: Network Visualization: Saliency Maps, Class Visualization, and Fooling Images (15 points)

#### Q4: Generative Adversarial Networks (15 points)

#### Q5: Self-Supervised Learning for Image Classification (20 points)

### 后记


当然除了以上的前言以外, 更多的疑问是：未来走向何方？

中国传统制造业在上世纪崛起，这个世纪逐渐边缘。机器学习也会边缘吗？
<!--我心里有一个答案：时机不知道。谁知道下一个伟人什么时候出现。但是另一方面，出现的分布倒是有的。-->

<!--想象力和创造力并且有行动力。到底在哪一个领域会实现？-->

再往前，倒退回2005年, 复盘一下触屏手机的出现。手机一直都有，但乔布斯让时代进入next level。一直到现在手机还在迭代更新，there is no endless。2015年cs231n的开课，直到今年2022图像领域依旧在百花齐放。同理, there is no endless, 因为这是人类认知世界的方式和工具。我的观点是：这个领域没有问题。就像查字典和搜索引擎一样, 只是方式变了，纸质变成了电子。照着人类的进化来说，只要眼睛还在，人类的感知没有进化，那么认知世界的方式也不会进化。只是接收信息的方法变化。

所以只要是人类不进化，现有的任何满足当前人类处理各种事情的方法和方式都不会"过时"。


<!--那其实很明显了，认识世界的方式，比如硬件包括投影,电池，新能源就是next generation. 这是进化的方向。-->



