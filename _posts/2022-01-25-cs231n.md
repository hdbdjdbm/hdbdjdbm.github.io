---
layout: post
title: the note of cs231n
date: 2022-01-25 17:39:00
description: å­¦ä¹ ç¬”è®°
---

### å‰è¨€

æ­¤ç¯‡ä¸º Stanford Winter Quarter 2016 class: CS231n: Convolutional Neural Networks for Visual Recognitionè¯¾ç¨‹ä¸ªäººç¬”è®°ã€‚

äº”å­æ£‹(MCTS)æ˜¯AlphaZeroçš„ä¸€ä¸ªæŠ›ç –å¼•ç‰ï¼Œå€Ÿæ­¤æœºä¼šå­¦ä¹ ä¸€ä¸‹ç¥ç»ç½‘ç»œã€‚

> äººç±»çš„æ„ŸçŸ¥50%ä»¥ä¸Šæ˜¯é€šè¿‡è§†è§‰å¤„ç†è·å–çš„ã€‚

è¿™ä¸ªç»“è®ºè®©æˆ‘å›å¿†èµ·å°å­¦ä¸‰å››å¹´çº§çš„æ—¶å€™å–œæ¬¢è·³çš®ç­‹ ;äº”å…­å¹´çº§å–œæ¬¢ç©4399çš„flashæ¸¸æˆ ;åˆä¸€åˆäºŒå¼€å§‹çœ‹å°è¯´ ;é«˜ä¸€é«˜äºŒå¼€å§‹çœ‹è§†é¢‘(è¿½æ˜Ÿ)ã€‚è¿™æ ·æ¢³ç†å‘ç°ä¸€åˆ‡éƒ½æ˜¯åœ¨æœç€â€œæ„ŸçŸ¥å¤šå…ƒåŒ–â€çš„æ–¹å‘å‘å±•ã€‚

å·ç§¯æ¨¡å‹åœ¨1998å¹´å°±å·²ç»è¢«LeCunæå‡ºã€‚ä½†å½“æ—¶ç¢äºæ•°æ®ä»¥åŠç¡¬ä»¶å¤§å®¶å¹¶ä¸çŸ¥é“è¿™ä¸ªæ¨¡å‹èƒ½work outã€‚

Professor Li(FeiFei Li) æŠŠæ•´ä¸ªdata-drivençš„modelç±»æ¯”ä¸ºå°å­©çš„æˆé•¿è¿‡ç¨‹ã€‚çœ¼ç›çœ‹åˆ°çš„viewä¸ºè¾“å…¥é¡¹ï¼Œæˆ‘ä»¬çš„åŸºå› å°±æ˜¯å·²ç»trainå¥½çš„modelã€‚ç”±äºæ•°æ®çš„é«˜ç»´åŒ–ï¼Œè®©äººä¸å¾—ä¸ä½¿ç”¨å¤šé‡å·ç§¯å»æ‹Ÿåˆé«˜ç»´ã€‚

åœ¨å¬å®Œè¯¾ç¨‹ä¹‹åï¼Œå‘ç°äº†æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç°çŠ¶ï¼špracticeæ€»æ˜¯èµ°åœ¨theoryçš„å‰é¢ã€‚

æˆ‘ä»¬å¸¸è¯´ç¥ç»ç½‘ç»œæ˜¯ä¸ªâ€œé»‘ç®±â€ï¼Œå¤§è‡´æœ‰ä¸¤å±‚æ„æ€ã€‚ä¸€ä¸ªæ˜¯è¯´ï¼Œæˆ‘ä»¬ä¸èƒ½åˆ»ç”»ç½‘ç»œå…·ä½“æ˜¯åœ¨åšä»€ä¹ˆï¼ˆæ¯”å¦‚åœ¨æ¯ä¸€å±‚æå–ä»€ä¹ˆæ ·çš„ç‰¹å¾ï¼‰ã€‚å¦ä¸€ä¸ªæ˜¯è¯´ï¼Œæˆ‘ä»¬ä¸çŸ¥é“å®ƒä¸ºä»€ä¹ˆåœ¨åšè¿™äº›ï¼Œä¸ºä»€ä¹ˆä¼šæœ‰æ•ˆã€‚

æœ‰ç½‘å‹æ‹¿ç‰›é¡¿è¢«è‹¹æœç ¸ä¸­ç±»æ¯”

> æ·±åº¦å­¦ä¹ æ˜¾ç„¶æ˜¯åšå¯¹äº†something, åœ¨æŸä¸ªè§’åº¦è§¦ç¢°åˆ°äº†çœŸç†ï¼Œä½†å¦‚æœä¸æ‰“å¼€é»‘ç®±ï¼Œæˆ‘ä»¬æ— æ³•çŸ¥é“å®ƒåˆ°åº•åšå¯¹äº†ä»€ä¹ˆï¼ŒçœŸç†æ˜¯ä»€ä¹ˆã€‚åœ¨ç‰›é¡¿ä¹‹å‰ï¼Œå¤§å®¶éƒ½è§åˆ°äº†è‹¹æœè½åœ°ã€‚ä½†åœ¨å½“æ—¶äººä»¬çš„è§†è§’ä¸­ä»–ä»¬ä¸€å®šè®¤ä¸ºï¼Œè‹¹æœè½åœ°ä¸æ˜¯å¾ˆè‡ªç„¶çš„å—ï¼Œéœ€è¦è§£é‡Šã€éœ€è¦çŸ¥é“ä¸ºä»€ä¹ˆå—ï¼Ÿå½“æ—¶çš„äººä»¬ä¹Ÿä¼šè®¤ä¸ºè§£é‡Šè¿™ç§ç°è±¡ç®€ç›´æ— ä»ä¸‹æ‰‹ã€‚è·Ÿä»Šå¤©çš„æ·±åº¦å­¦ä¹ æœ‰ç‚¹åƒå§ï¼Ÿä½†æ˜¯å½“ç‰›é¡¿å‘Šè¯‰æˆ‘ä»¬ä¸ºä»€ä¹ˆè‹¹æœä¼šè½åœ°ä¹‹åï¼Œä¸–ç•Œä»æ­¤å°±ä¸ä¸€æ ·äº†ã€‚

æ·±åº¦å­¦ä¹ ç›®å‰å°±å¥½æ¯”å®éªŒç‰©ç†ï¼Œæœ‰å¾ˆå¤šå®éªŒè§‚å¯Ÿï¼Œä½†æ²¡æœ‰ç†è®ºèƒ½å¤Ÿè§£é‡Šã€‚ä¹Ÿè·Ÿç‰©ç†å­¦çš„å‘å±•ä¸€æ ·ï¼Œå®éªŒæ€»æ˜¯èµ°åœ¨å‰é¢ã€‚

éšç€äººä»¬æ¥è§¦çŸ¥è¯†çš„é—¨æ§›è¶Šæ¥è¶Šä½(è¿™æ˜¯å¥½äº‹)ï¼Œä¸æ–­æœ‰å¾ˆå¤šæå‡æ•ˆæœçš„trickå‡ºç°ï¼Œæ¯”å¦‚Reluæ›¿æ¢sigmodå‡½æ•°ï¼Œ batch traningå’Œ normalizationæå‡åˆ¤åˆ«èƒ½åŠ›ï¼Œdrop out ,åˆ©ç”¨ä¸€äº›loss fuctionç­‰ç­‰ã€‚æœ‰äº›æ— è°“çš„æå‡æ¥æºäºç»éªŒï¼Œä½†æ˜¯æœ‰ä¸€äº›æ¥æºäºäººç±»å·²ç»æ„å»ºçš„è®¤çŸ¥ã€‚

å®è·µå’Œç†è®ºæ˜¯åˆ†ä¸å¼€çš„ï¼Œå®è·µä¿ƒè¿›å¯¹ç†è®ºçš„æ€è€ƒï¼Œç†è®ºå¼•å¯¼å®è·µçš„å‘å±•ã€‚å°±åƒå­¦ç•Œå’Œå·¥ä¸šç•Œçš„å…³ç³»ã€‚æˆ‘ä»¬æ—¢è¦å±•ç¤ºä¸€ä¸ªä¸œè¥¿èƒ½work out, æ›´è®¨è®ºä¸ºä»€ä¹ˆèƒ½work outã€‚

æˆ‘çš„ç°æœ‰æ„Ÿè§‰æ˜¯ï¼šè¿™ä¸ªè¿‡ç¨‹æ˜¯ä»é«˜ç»´æ•°æ®æå–ä½ç»´ç‰¹å¾ã€‚UCBçš„professor Ma ä¹Ÿæåˆ°è¿‡è¿™ä¸ªç‚¹(å¼ºè¡Œç¢°ç“·äº†å±äºæ˜¯ï¼Œå†™åœ¨todo-list, Mayiè€å¸ˆçš„é«˜ç»´æ•°æ®ä¸ä½ç»´æ¨¡å‹)ã€‚ç±»ä¼¼äººç±»åŒºåˆ†çŒ«å’Œç‹—ï¼Œå¦‚æœæœ‰nä¸ªç‰¹å¾ï¼šä¸€çœ‹åŸè¾¹å½¢çŠ¶ï¼ŒäºŒçœ‹çº¹ç†ï¼Œä¸‰çœ‹é¢œè‰²ï¼Œå››çœ‹...ã€‚åˆ°åº•å¤šå°‘çœ‹èƒ½åŒºåˆ†å‡ºçŒ«å’Œç‹—,æ¯ä¸€ä¸ªçœ‹æ˜¯å¦èƒ½ä»é«˜ç»´çŸ©é˜µä¸­æå–å‡ºæ¥ã€‚å½“ç„¶è¿™å¹¶ä¸æ˜¯å®¹æ˜“çš„ã€‚CS231nåé¢çš„ä¸€ä¸ªlectureå°±åœ¨è®¨è®ºæ¯ä¸€å±‚çš„æ„ä¹‰æ˜¯çœ‹ä¸å¤ªå‡ºæ¥çš„(transformeræ”¹è¿›äº†è¿™ä¸€ç‚¹)ã€‚

å½“æˆ‘ä»¬çš„è¯†åˆ«ä»0åˆ°1è¿ˆå‡ºä¸”åˆ°è¾¾äº†ä¹‹å,å°±ä¼šä»1-100é£é€Ÿå¿«è¿›ï¼Œè¿˜æ˜¯æ‹¿å°æœ‹å‹ç±»æ¯”ï¼šæ­£å¸¸å°æœ‹å‹3å²èƒ½è¯†å­—ï¼Œ5å²èƒ½èƒŒè¯—ã€‚çªç„¶æœ‰ä¸ªå°æœ‹å‹3å²å°±èƒ½èƒŒè¯—,å®£å¸ƒè¿™ä¸ªç»“æœåï¼Œæœ‰äº›å°æœ‹å‹2å²å°±èƒ½èƒŒè¯—(å¤¸å¼ )ã€‚ğŸ’»åœ¨ç›®å‰å‘å±•é˜¶æ®µä¸å­˜åœ¨å¤§é‡çš„ä¼¦ç†é—®é¢˜(æ­¤æ—¶å¿…é¡»æåˆ°2001å¤ªç©ºæ¼«æ¸¸ç”µå½±é‡Œé¢çš„HAL9000å¯¹äººç±»è¯´çš„è¯ï¼šå¦‚æœæœ‰é—®é¢˜ï¼Œä¸€å®šæ˜¯äººç±»çš„é—®é¢˜)ï¼Œæ‰€ä»¥äººä»¬ä¼šç«­å°½æ‰€èƒ½è®©ä¸‰ä¸ªæœˆçš„â€œå­©å­â€å­¦ä¼šèƒŒè¯—ã€‚

æœ€è¿‘è¿™ä¸€ä¸ªæœˆæ²‰è¿·ç§‘å¹»ç”µå½±æ— æ³•è‡ªæ‹”,ä¸€ç›´æƒ³å†™ä¸€äº›è§‚åæ„Ÿ,ä½†ä¸‹ç¬”çš„æ—¶å€™æ€»æ‰¾ä¸åˆ°ä¸€ä¸ªåˆé€‚çš„åˆ‡å…¥ç‚¹ã€‚ç»“æœåœ¨å­¦ä¹ æœ‰å…³çš„çŸ¥è¯†ç‚¹çš„æ—¶å€™,åè€Œåˆæœ‰äº†å¾ˆå¤šèƒ½æŠ’å‘çš„è§’åº¦ã€‚(æ­¤å¥è¯å¯ä»¥ç¿»è¯‘ï¼šåœ¨å­¦ä¹ çš„æ—¶å€™,å…¶ä»–ä¸œè¥¿éƒ½å¾ˆæœ‰æ„æ€)

å¦å¤–ï¼Œæˆ‘ä¸€ç›´å¯¹æ»¥ç”¨å†…å·ä¿æœ‰å¦å®šæ€åº¦ï¼Œå¯¹äºæ‰€æœ‰èº«å¤„å­¦æœ¯ç•Œçš„åŒå­¦ä»¬(å“ªæ€•åªæ˜¯ç¢°åˆ°äº†å­¦æœ¯ç•Œçš„å¤§é—¨)æ›´åº”è¯¥è®¤è¯†åˆ°è¿™ä¸€ç‚¹ã€‚åœ¨å‰äººåŸºç¡€æ¡†æ¶æ‰“å¥½ä¹‹ååäººè¦åšçš„å°±æ˜¯stand on the shoulders of giant.å­¦æœ¯ç•Œçš„å†…å·æ˜¯äººç±»æ›´æ–°è¿­ä»£çš„å¿…è¦æ¡ä»¶ã€‚LeCunä¹Ÿæœªæ›¾çŸ¥åå¹´åå·ç§¯æ¨¡å‹ä¼šæ¨åŠ¨ä¸–ç•Œçš„è¿›æ­¥ï¼Œæˆ‘ä»¬åº”è¯¥ä¿æœ‰ä½¿å‘½æ„Ÿ(ä½†è¦æ¸…æ¥šè®¤çŸ¥è‡ªå·±ä¹Ÿæ˜¯æ™®é€šäºº)ã€‚

æœ€åï¼Œæ„Ÿè°¢æ‰€æœ‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå‘å…‰å‘çƒ­çš„äººä»¬ã€‚


### Lecture 1 Introduction

è§†è§‰ä¿¡æ¯ åƒç´ ä¿¡æ¯

"ä¸€äº›å…³é”®è¯è®°å½•"

* å†å²å…ˆé©±(1981è¯ºè´å°”åŒ»å­¦å¥–)
æ¯ä¸€åˆ—çš„ç¥ç»å…ƒåªè¯†åˆ«æŸä¸€ç§ç‰¹å®šçš„è¾¹ç¼˜
* Block world(1966,MIT)
äººä»¬è¯†åˆ«ç‰©ä½“æ˜¯åŸºäºåŸºäºç‰©ä½“çš„è¾¹ç¼˜å’Œå½¢çŠ¶ã€‚
* David Marrï¼š
visual start with a simple 
visual is  HIERARCHICA(åˆ†å±‚)
-->è¾¹ç¼˜ç»“æ„ï¼Œ2.5Dï¼Œ3D
* normlized cut
* Face Detection
* features(ç‰¹å¾)
* part model
* PASCAL Visual Object Challenge
* 2012 å¹´çš„Image Classification Challengeä½¿ç”¨äº†å·ç§¯ç½‘ç»œç‰¹å¾+SVMï¼Œ7å±‚ã€‚æ€è·¯å’Œ1998Yann LeCunå‡ ä¹ä¸€æ ·ã€‚ä½†é‚£ä¸ªæ—¶å€™ç¡¬ä»¶æœ‰é™åˆ¶ã€‚
* 2015æ·±åº¦æ®‹å·®ç½‘ç»œ(151å±‚)
  
æœ€ç»ˆæ„¿æƒ³çš„output:çœ‹å›¾è®²æ•…äº‹(RNN)


### Lecture 2&3 å›¾åƒåˆ†ç±»

**The problem: semantic gap**

Eg:300 x 100 x 3
* 0-255,ä¸‰ç»´(äº®åº¦ï¼Œä½ç½®ï¼ŒRGB)æ•°æ®è®°å½•å±€éƒ¨å›¾åƒã€‚
1. ç”±äºäº®åº¦ä¼šå˜åŒ–ï¼Œéœ€è¦**é²æ£’æ€§**çš„ç®—æ³•å®ç°ç¨³å®šæ€§
2. å˜å½¢deformation
3. background clutter ç­‰ç­‰

solution: data-driven approach

#### kNN classifier

* key point: æ¯”è¾ƒä¸¤ä¸ªå›¾åƒï¼šè·ç¦»çš„åº¦é‡ ,the choice of distance and k are hyperpearameters(è¶…å‚æ•°)
--å–å†³äºé—®é¢˜--è®¾ç½®fold - cross validationã€‚Split data into folds,try each fold as validation and average the resultsã€‚

* notice: Useful for small datasets, but not used too frequently in deep learning. And KNN with pixel distance never used: (1) ä¸€æ–¹é¢åœ¨é«˜ç»´åº¦è·ç¦»æœ¬èº«æ²¡æœ‰æ„ä¹‰(å¯è§ä¸Šä¼ çš„å¦ä¸€ç¯‡final paper)ã€‚(2) å¹¶ä¸”è·ç¦»æä¾›çš„åº¦é‡ä¸å¤ªå‡†ç¡®ã€‚(3) testæ—¶é—´æ¯”è¾ƒé•¿

* features
   (1) è·ç¦»çš„åº¦é‡æœ‰ä¸åŒçš„é€‰æ‹©ï¼Œæ¯”å¦‚ $L_1$ å’Œ $L_2$ ã€‚éœ€è¦æ³¨æ„ä¸åŒè·ç¦»çš„æ€§è´¨ã€‚
   (2) åœ¨å¯¹çŸ©é˜µè¿›è¡Œå¤„ç†çš„æ—¶å€™æ³¨æ„numpyçš„å¤„ç†ï¼Œæ¯”å¦‚axiså’Œkeepdimï¼Œè¿˜æœ‰ä¸€äº›ç®€ä¾¿è¿ç®—çš„å‡½æ•°æ¯”å¦‚bincount,argmaxç­‰ã€‚
   (3) KNNå¹¶ä¸æ˜¯çº¿æ€§åˆ†ç±»å™¨ï¼Œè€Œæ˜¯å°†ç©ºé—´åˆ†ä¸ºå‡¸å¤šè¾¹å½¢ã€‚

* å¯ä¾›æ‹“å±•: å¯¹äºKNNåˆ†ç±»å™¨æ¥è¯´ã€‚trainéœ€è¦O(1),ä½†æ˜¯predictéœ€è¦ O(N). å¾ˆBADï¼Œæ‰€ä»¥æœ‰ä¸€äº›fast/approximateçš„æ–¹æ³•ï¼Œæ¯”å¦‚k-d treeæˆ–è€… ball- tree.


#### çº¿æ€§åˆ†ç±»

![image-1](https://pic.imgdb.cn/item/61dd3a492ab3f51d91818262.png)

![image-2](https://pic.imgdb.cn/item/61dd3a492ab3f51d91818266.png)

W: reshapingï¼Œåªæ˜¯åœ¨è®¡æ•°ï¼Œæ¶ˆé™¤å¤±çœŸã€‚ 

* key point: ç”¨çº¿æ€§å‡½æ•°åˆ†éš”ç©ºé—´

* features
Interpreting a Linear Classifierï¼šWæ˜¯ç±»åˆ«çš„çŸ©é˜µ

* quantifing what it seems to have a god W:Loss function

#### SVM loss

the SVM loss has the form:

$$
L_i = \sum_{j\neq y_i}max(0,s_j - s_{y_i} + 1)
$$

$$
f(x,W)=  Wx
$$

$$
L = \frac{1}{N}\sum_{i=1}^N\sum_{j\neq y_i}max(0,f(x_i;W)_j-f(x_i;W)_{y_i} +1 )
$$

w is not unique, we want regularization:

$$
L(W) = \frac{1}{N}\sum_{i=1}^{N}L_i(f(x_i,W),y_i) + \lambda R(W)
$$


å‰è€…æ˜¯Data loss: Model predictions
should match training data

åè€…æ˜¯Regularization: Prevent the model
from doing too well on training dataï¼Œ$\lambda$ is regularization strength
(hyperparameter)ã€‚$R(W)$ = $\sum\sum w^2$





#### softmax loss(Multinamial Logistic Regression)

$$
s = f(x_i;W)
$$

$$
P(Y=k|X=x_i) = \frac{e^sk}{\sum_j e^{sj}}
$$

$$
L_i = -log(\frac{e^{sy_i}}{\sum_j e^{sj}})
$$

**SVM loss VS softmax loss**



* å¦‚ä½•æœ€å¿«çš„æ‰¾åˆ°å‘¢ï¼š
#### Optimization

numerical fradient
&
analytical gradient

> always use the analytical gradient but to check the numerical gradient this is called gradient check


Stochastic Gradient Descent (SGD)

Approximate sum
using a minibatch of examples

### Lecture 4 Neural Networks and Backpropagation

questionï¼šHow to find the best W $\triangledown_WL$


#### features of pictures
Color Histogram and Histogram of Oriented Gradients (HoG)

#### Neural Networks

* defination

åˆ†å±‚è®¡ç®—

![image-5](https://pic.imgdb.cn/item/61eebaff2ab3f51d9195a978.png)


* activation function

æ¿€æ´»å‡½æ•°

![image-6](https://pic.imgdb.cn/item/61eebbb32ab3f51d91966888.png)
(ReLU is a good default
choice for most problems)

* training a 2-layer Neural Network needs:Define the network,Forward pass,Calculate the analytical gradients

* Setting the number of layers and their sizes
![image-7](https://pic.imgdb.cn/item/61eebd4c2ab3f51d91980d2a.png)

![image-8](https://pic.imgdb.cn/item/61eebd4c2ab3f51d91980d31.png)

$$
s = f(x;W_1,W_2) = W_2max(0,W_1x)
$$ 
Nonlinear score function
$$
L_i = \sum_{j\neq y_i}max(0,s_j - s_{y_i} + 1)
$$
SVM Loss on predictions
$$
R(W) = \sum_k W_k^2
$$
Regularization
$$
L = \frac{1}{N}\sum_{i=1}^N L_i + \lambda R(W_1) + \lambda R(W_2)
$$

question: compute the gradient of $W_1$ and $W_2$
solution: Backpropagation

![image-9](https://pic.imgdb.cn/item/61eebf462ab3f51d919a43de.png)



### Lecture 5 setup notice

#### Mini-batch SGD
![image-5](https://pic.imgdb.cn/item/61f3eaaa2ab3f51d912c093d.png)
1. Sample a batch of data
2. Forward prop it through the graph, get loss
3. Backprop to calculate the gradients
4. Update the parameters using the gradient

#### activation functions
1. Sigmoid

   $$
   \sigma(x) = \frac{1}{1+e^{-x}}
   $$


   festure:å€¼åŸŸåœ¨[0,1]
   DISAD: 
   Saturated neurons â€œkillâ€ the gradients; Sigmoid outputs are not zerocentered(å¦‚æœè¾“å…¥æ˜¯+çš„,é‚£ä¹ˆgradientä¹Ÿæ˜¯æ­£çš„); 
   exp() is a bit compute expensive;

2. tanh(x)
   Squashes numbers to range [-1,1]
   zero centered (nice)
   still kills gradients when saturated



3. ReLU(Rectified Linear Unit)
   
   $$
   f(x) = max(0,x)
   $$

   AD:
   Converges much faster than sigmoid/tanh in practice

   DISAD:
   not zero-centered output
   an annoyance

4. Leaky ReLU
   
   $$
   f(x) = max(0.01x,x)
   $$

   Parametric Rectifier (PReLU):
   $$
   f(x) = max(ax,x)
   $$

5. ELU
   
   $$
   if x>0: x
   if x <= 0: \alpha(exp(x)-1)
   $$

6. Maxout 
   $max(w_1^Tx+b_1,w_2^Tx+b_2)$

summary:
- Use ReLU. Be careful with your learning rates
- Try out Leaky ReLU / Maxout / ELU
- Try out tanh but donâ€™t expect much
- **Donâ€™t use sigmoid**

#### data processing

some examples

![image-10](https://pic.imgdb.cn/item/61f3e2c42ab3f51d912255a8.png)

![image-11](https://pic.imgdb.cn/item/61f3e2c42ab3f51d912255ad.png)

* it's common to zero-centered data but not normalized(why?)
* use PCA and whitening(common in ML not in image)
* substract the maen image - substract per-cha nel mean

#### weight initialization

* W=0 is a bad idea causeæ¯ä¸€ä¸ªç¥ç»å…ƒè¡¨ç°çš„ä¸€æ ·ï¼Œä¸å¥½å¯»æ‰¾gradient
  
* instaed W = 0.01 * np.random.randn(D,H) (å¯¹small networks are okay, but can lead to non-homogeneous distributions of activsations across the layers of a network)

* when debug, sometimes try 0.1* np.random,rand(D,H)

#### Batch Normalization

![image-13](https://pic.imgdb.cn/item/61f3eaaa2ab3f51d912c0941.png)

AD:Improves gradient flow ;Allows higher learning rates ;Reduces the strong dependence on initialization ;Acts as a form of regularization ;slightly reduces the need for dropout

éœ€è¦æ³¨æ„å’Œdata processing çš„normlizeéƒ½æ˜¯åœ¨normlize, ç›®çš„ä¸ä¸€æ ·,å‰è€…æ˜¯ä¸ºäº†SGD


step to train :
* Step 1: Preprocess the data
* Step 2: Choose the architecture
  Double check that the loss is reasonable
  loss went up, good. (sanity check)


#### babysitting the learning process

* Double check that the loss is reasonable:
* Make sure that you can overfit very small portion of the training data(100% accurancy we said overfit the data)

tips:

loss not going down:learning rate too low

loss exploding:learning rate too high

#### Hyperparameter Optimization

Cross-validation strategy

reg and lr: itâ€™s best to optimize
in log space


* gap between training and validation:
   big gap = overfitting
   => increase regularization strength

   no gap
   => increase model capacity?

* Evaluation
model ensembles

### Lecture 6 continue setup notice

#### parameter update

ä¸€äº›ä¸ªä¸‹é™æ–¹æ³•ï¼š
* momentum update(soleve the SGD slow)
* Nesterov Monmentum update
* nag
* AdaGrad update
* PMSProp update
* Adam update(looks a bit like RMSProp with momentum)
* second order optimization methods(no hyperparameters)
* BFGS(Quasi-Newton methods)
* L-BFGS(some features)

**summary**
Adam is good default choice

If you can afford to do full batch updates then try out
L-BFGS 

#### Evaluation:
Model Ensembles

Train multiple independent models,At test time average their results

> Enjoy 2% extra performance

#### Regularization (dropout)
![image-1](https://pic.imgdb.cn/item/61f3ef922ab3f51d91318849.png)

reason:
* Forces the network to have a redundant representation
* Dropout is training a large ensemble of models (that share parameters).

drop in forward pass,scale at test time

#### explanation about the parameter

learning rate(å†³å®šäº†æˆ‘ä»¬è®­ç»ƒç½‘ç»œé€Ÿç‡çš„å¿«æ…¢,how which step we used?)


regularization(dropout)
* no need so many features
* å¦ä¸€ç§å½¢å¼çš„ensemble
*  Inberted fropout

#### Convolutional Neural Networks


### Lecture 7 convolution neural networkçš„ç»“æ„


#### Convolution Layer

ä¸€äº›çœ‹å›¾è¯´è¯ç¯èŠ‚ï¼š

åæ­£å°±æ˜¯å¤šç»´ç©ºå˜æ¢,æ‰¾åˆ°ä¸€äº›ç‰¹å¾

![image-1](https://pic.imgdb.cn/item/61f3fc9a2ab3f51d9140f0cf.png)

![image-2](https://pic.imgdb.cn/item/61f3fc9a2ab3f51d9140f0d3.png)

![image-3](https://pic.imgdb.cn/item/61f3fc9a2ab3f51d9140f076.png)

![image-4](https://pic.imgdb.cn/item/61f3fc9a2ab3f51d9140f07d.png)


A closer look at spatial dimensions:å¯ä»¥changeä¸€äº›strideå’Œpad, ç”±äºä¼šå‹ç¼©, å¯ä»¥åœ¨è¾¹ç¼˜åŠ ä¸€äº›0

#### Pooling layer

* makes the representations smaller and more manageable
* operates over each activation map independently
![image-2](https://pic.imgdb.cn/item/61f3fef32ab3f51d9143f1bc.png)


#### fully connected layer
Contains neurons that connect to the entire input volume, as in ordinary Neural Networks


take a look:

![image-3](https://pic.imgdb.cn/item/61f3fef32ab3f51d9143f1b8.png)

#### case study

LeNet(1998)
AlexNet(2012)
ZFNet(2013)
VGG(2014)
GooLeNet(2014)
ResNet(2015)

all there are used to clssification

### Lecture 8 Localization and Detection

![image-1](https://pic.imgdb.cn/item/61f4017d2ab3f51d91473cc5.png)

#### claassification + Localization

* classification
input:image
output:class label
evaluation metric:accuracy

* Localization

input:image
output:box in the image(x,y,w,h)
evaluation metric:intersection over the Union
Classification + Localization: Do both


Idea #1: Localization as Regression

ä½¿ç”¨å›å½’output box coordinate

![image-1](https://pic.imgdb.cn/item/61f4031e2ab3f51d91497e1b.png)


input:image --(Neural Net)--> output:Box coordinates(4 numbers)

compare with Correct output:box coordinates(4 numbers)


Idea #2 sliding window

ä½¿ç”¨çª—å£åˆ†ç±»

![image-2](https://pic.imgdb.cn/item/61f4031e2ab3f51d91497e12.png)

* Run classification + regression network at multiple locations on a highresolution image


#### detection

å¦‚æœä½¿ç”¨regressionåˆ™å‚æ•°å°±å¤ªå¤šäº†

Detection as Classification

Problem: Need to test many positions and scales

Solution: If your classifier is fast enough, just do it

Before CNNs:

2005 Histogram of Oriented Gradients(HoG)

2010 deformable parts modek(DPM)

* detection as clssification
region proposals: selective search


putting it together:R-CNN
step:
1. Train (or download) a classification model for ImageNet(AlexNet)
2. Fine-tune model for detection
3. extract features
4. Train one binary SVM per class to classify region features
5. bbox regression

Evaluation: mAP

problem:slow at test time,

solution:
fast R-CNN
fasyer R-CNN

### Lecture 9 deep understanding

layersè¶Šå¾€åè¶Šä¸å¥½è§£é‡Š

t-SNE visualization

Decinv approaches 

optimaize the image

ä¸€äº›trick

#### deep dream

#### Neural Style

#### Adversarial Examples

### Lecture 10 RNN

è¿™ä¸€å—æš‚ç•¥, æš‚æ—¶æ°¸ä¸å¤ªä¸Šã€‚

#### Recurrent Neural Networks


![image-1](https://pic.imgdb.cn/item/61f40be02ab3f51d9155e8c9.png)

one to one: Vanilla Neural Networks

one to many: Image Captioning(image -> sequence of words)

many to one: Sentiment Classification sequence of words -> sentiment

many to many:Machine Translation(seq of words -> seq of words)

many to many: Video classification on frame level

#### LSTM

Long short Term Memory(1997)



### Lecture 11 practice with CNN


#### data augmentation
(useful for small dataset)
* horizontal flips
* random crops/scales
* color jitter

#### transfer learning
can freeze some parameter just train the last few layers

![image-1](https://pic.imgdb.cn/item/61f40f292ab3f51d915aad27.png)

![image-2](https://pic.imgdb.cn/item/61f40f292ab3f51d915aad2b.png)

#### how to stack the convolution

* Layers :change the size of ä»¿å°„ç»“æœ

* im2col

#### computing convolution
* n2cool:easy to implement, but big memory overhead
* FFT:big speeduos for small kernels
* fast algorithms seem promising, not wideljy used

#### GPU,CPU

å¹¶è¡Œå’Œæµ®ç‚¹æ•°


### Lecture 12 sogtware packages

#### Caffe

from U.C. Berkely

Main classes 
* Blob: stores data and derivatives
* Layer:Trandforms bottom blobs to top blobs
* Net: many layers; computes gradients
* solver:uses gradient toupdate weights

steps:
1. convert data
2. define net
   write prototxt
3. define solver
   write prototxt
4. train

caffe:model zoo

Pros/Cons
+ good for feedforward
- need to write C++
- not good recurrent networks

#### Torch

from NYU
wriiten in C and Lua

Torch: Tensors

Torch: nn
nn module lets you easily build and train neural nets
picture

Torch: cunn
picture

Torch: optim
picture

Torch: Modules

writing your own modules is easy

Torch: nngraph

Pros/Cons
- not great for RNNs
- 


#### Theano

#### TensorFlow

from Google

TensorFlow:Tensorboard
way to visualize what's happening inside your odels



### Lecture 13 

#### Segmentation

pictures

* semantic segmentation
label every pixel in the image(repeat for clssification for every picel)
unsample
unpooling
skip connections can help

* instance segmentation
very similiar to R-CNN
detect instance, generate mask
similar pipelines to object detection
 
region classification
region refinement

do logistic regression to classify


#### soft attention


soft attention for translation
soft attention for other field

#### hard attention
pass
need reinforcement learning

### Lecture 14 Videos and Unsupervised Learning
not done yet


### assignment1

#### Q1 K-Nearest Neighbor classifier

```python
class KNearestNeighbor(object):
    def __init__(self):
        pass

    def train(self,X,y):
        self.X_train = X
        self.y_train = y

    def predict(self, X, k = 1, num_loops = 0):
        if num_loops == 0:
            dists = self.compute_distances_no_loops(X)
        if num_loops == 1:
            pass
        return self.predict_labels(sidts, k=k)
    
    def compute_distances_two_loops(self, X):
        pass

    def predict_labels(self,dists,k=1):
        pass
```

æ³¨æ„numpyçš„åº”ç”¨å’Œboardcasting, **å°‘ç”¨å¾ªç¯**ã€‚


#### Q2&3 Training a Support Vector Machine & Implement a Softmax classifier

> subtract the mean image from train and test data


```python

class LinearClassifier(object):
    def __init__(self):
        self.W = None
    def train(self, X, y, learning_rate=1e-3, reg=1e-5,num_iters=100,batch_size=200, verbose=False):
        pass
    
    def predict(self,X):
        pass

    def loss(self, X_batch, y_batch, reg):
        pass



class LinearSVM(LinearClassifier):
    def loss(self, X_batch, y_batch, reg):
        return svm_loss_vectorized(self, X_batch, y_batch, reg)
    
    
class Softmax(LinearClassifier):
    def loss(self, X_batch, y_batch, reg):
        return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)



```

åŒæ ·, æ³¨æ„å‘é‡çš„åº”ç”¨.


#### Q4 Two-Layer Neural Network

```python

class TwoLayerNet(object):
    def __init__(self,input_size, hidden_size, output_size, std=1e-4):

        self.params = {}
        self.params['W1'] = std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def loss(self, X, y=None, reg=0.0):
        pass

    def train(self, X, y, X_val, y_val,learning_rate=1e-3, learning_rate_decay=0.95,reg=5e-6, num_iters=100,batch_size=200, verbose=False):
        pass

    def prefict(self,X):
        pass
        
```

æ³¨æ„åå‘ä¼ æ’­çš„part.

#### Q5 Higher Level Representations: Image Features

hog_feature 
color_histogram_hsv

### assignment2


#### Q1 Multi-Layer Fully Connected Neural Networks 

```python

class TwoLayerNet(object):
    pass
class FullyConnectedNet(object):
    pass
```

trainåˆæ˜¯å¦ä¸€ä¸ªä¸“é—¨çš„ç±»ï¼Œcause modelæœ‰å¾ˆå¤š, ä½†trainçš„åŠ¨ä½œéƒ½æ˜¯ç›¸ä¼¼çš„ã€‚

#### Q2 Batch Normalization 

> with batch normalization we can avoid the problem of vanishing and exploding gradients because it normalizes every affine layer (xW+b), avoiding very large/small values. Moreover, its regularization properties allow to decrease overfitting.

> the batch size affects directly the performance of batch normalization (the smaller the batch size the worse). Even the baseline model outperforms the batchnorm model when using a very small batch size. This problem occurs because when we calculate the statistics of a batch, i.e., mean and variance, we try to find an approximation of the statistics of the entire dataset. Therefore with a small batch size, these statistics can be very noisy. On the other hand, with a large batch size we can obtain a better approximation.

other idea: **Layer Normalization**


#### Q3 Dropout 

dropoutå¯ä»¥å‡å°‘ä¸€äº›è¿‡æ‹Ÿåˆç°è±¡

#### Q4 Convolutional Neural Networks

è®°å½•ä¸€äº›å°trick

#### Q5 PyTorch/TensorFlow on CIFAR-10 

We want you to stand on the shoulders of giants!


### assingment3

è¿™ä¸€å—æš‚æ—¶è¿˜æ²¡æœ‰ç›®çš„æ€§, åç»­è¡¥

#### Q1: Image Captioning with Vanilla RNNs (30 points)

#### Q2: Image Captioning with Transformers (20 points)

#### Q3: Network Visualization: Saliency Maps, Class Visualization, and Fooling Images (15 points)

#### Q4: Generative Adversarial Networks (15 points)

#### Q5: Self-Supervised Learning for Image Classification (20 points)

### åè®°


å½“ç„¶é™¤äº†ä»¥ä¸Šçš„å‰è¨€ä»¥å¤–, æ›´å¤šçš„ç–‘é—®æ˜¯ï¼šæœªæ¥èµ°å‘ä½•æ–¹ï¼Ÿ

ä¸­å›½ä¼ ç»Ÿåˆ¶é€ ä¸šåœ¨ä¸Šä¸–çºªå´›èµ·ï¼Œè¿™ä¸ªä¸–çºªé€æ¸è¾¹ç¼˜ã€‚æœºå™¨å­¦ä¹ ä¹Ÿä¼šè¾¹ç¼˜å—ï¼Ÿ
<!--æˆ‘å¿ƒé‡Œæœ‰ä¸€ä¸ªç­”æ¡ˆï¼šæ—¶æœºä¸çŸ¥é“ã€‚è°çŸ¥é“ä¸‹ä¸€ä¸ªä¼Ÿäººä»€ä¹ˆæ—¶å€™å‡ºç°ã€‚ä½†æ˜¯å¦ä¸€æ–¹é¢ï¼Œå‡ºç°çš„åˆ†å¸ƒå€’æ˜¯æœ‰çš„ã€‚-->

<!--æƒ³è±¡åŠ›å’Œåˆ›é€ åŠ›å¹¶ä¸”æœ‰è¡ŒåŠ¨åŠ›ã€‚åˆ°åº•åœ¨å“ªä¸€ä¸ªé¢†åŸŸä¼šå®ç°ï¼Ÿ-->

å†å¾€å‰ï¼Œå€’é€€å›2005å¹´, å¤ç›˜ä¸€ä¸‹è§¦å±æ‰‹æœºçš„å‡ºç°ã€‚æ‰‹æœºä¸€ç›´éƒ½æœ‰ï¼Œä½†ä¹”å¸ƒæ–¯è®©æ—¶ä»£è¿›å…¥next levelã€‚ä¸€ç›´åˆ°ç°åœ¨æ‰‹æœºè¿˜åœ¨è¿­ä»£æ›´æ–°ï¼Œthere is no endlessã€‚2015å¹´cs231nçš„å¼€è¯¾ï¼Œç›´åˆ°ä»Šå¹´2022å›¾åƒé¢†åŸŸä¾æ—§åœ¨ç™¾èŠ±é½æ”¾ã€‚åŒç†, there is no endless, å› ä¸ºè¿™æ˜¯äººç±»è®¤çŸ¥ä¸–ç•Œçš„æ–¹å¼å’Œå·¥å…·ã€‚æˆ‘çš„è§‚ç‚¹æ˜¯ï¼šè¿™ä¸ªé¢†åŸŸæ²¡æœ‰é—®é¢˜ã€‚å°±åƒæŸ¥å­—å…¸å’Œæœç´¢å¼•æ“ä¸€æ ·, åªæ˜¯æ–¹å¼å˜äº†ï¼Œçº¸è´¨å˜æˆäº†ç”µå­ã€‚ç…§ç€äººç±»çš„è¿›åŒ–æ¥è¯´ï¼Œåªè¦çœ¼ç›è¿˜åœ¨ï¼Œäººç±»çš„æ„ŸçŸ¥æ²¡æœ‰è¿›åŒ–ï¼Œé‚£ä¹ˆè®¤çŸ¥ä¸–ç•Œçš„æ–¹å¼ä¹Ÿä¸ä¼šè¿›åŒ–ã€‚åªæ˜¯æ¥æ”¶ä¿¡æ¯çš„æ–¹æ³•å˜åŒ–ã€‚

æ‰€ä»¥åªè¦æ˜¯äººç±»ä¸è¿›åŒ–ï¼Œç°æœ‰çš„ä»»ä½•æ»¡è¶³å½“å‰äººç±»å¤„ç†å„ç§äº‹æƒ…çš„æ–¹æ³•å’Œæ–¹å¼éƒ½ä¸ä¼š"è¿‡æ—¶"ã€‚


<!--é‚£å…¶å®å¾ˆæ˜æ˜¾äº†ï¼Œè®¤è¯†ä¸–ç•Œçš„æ–¹å¼ï¼Œæ¯”å¦‚ç¡¬ä»¶åŒ…æ‹¬æŠ•å½±,ç”µæ± ï¼Œæ–°èƒ½æºå°±æ˜¯next generation. è¿™æ˜¯è¿›åŒ–çš„æ–¹å‘ã€‚-->



